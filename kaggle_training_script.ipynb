{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/huggingface/diffusers\nimport os \nos.chdir('diffusers')\n!pip install .\n\nos.chdir('examples/controlnet')\n!pip install -r requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T18:13:22.633539Z","iopub.execute_input":"2024-06-24T18:13:22.634076Z","iopub.status.idle":"2024-06-24T18:14:11.428223Z","shell.execute_reply.started":"2024-06-24T18:13:22.634019Z","shell.execute_reply":"2024-06-24T18:14:11.427120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate.utils import write_basic_config\n\nwrite_basic_config()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:14:11.430198Z","iopub.execute_input":"2024-06-24T18:14:11.430505Z","iopub.status.idle":"2024-06-24T18:14:16.883687Z","shell.execute_reply.started":"2024-06-24T18:14:11.430477Z","shell.execute_reply":"2024-06-24T18:14:16.882734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:14:16.884920Z","iopub.execute_input":"2024-06-24T18:14:16.885711Z","iopub.status.idle":"2024-06-24T18:14:34.026876Z","shell.execute_reply.started":"2024-06-24T18:14:16.885675Z","shell.execute_reply":"2024-06-24T18:14:34.025745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from huggingface_hub import notebook_login\n#notebook_login()\nfrom huggingface_hub import login\n\n\ntoken = \"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\"\nlogin(token=token)\n# ","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:14:34.028407Z","iopub.execute_input":"2024-06-24T18:14:34.028759Z","iopub.status.idle":"2024-06-24T18:14:34.144409Z","shell.execute_reply.started":"2024-06-24T18:14:34.028726Z","shell.execute_reply":"2024-06-24T18:14:34.143516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!accelerate env","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:14:34.146704Z","iopub.execute_input":"2024-06-24T18:14:34.146988Z","iopub.status.idle":"2024-06-24T18:14:41.155387Z","shell.execute_reply.started":"2024-06-24T18:14:34.146964Z","shell.execute_reply":"2024-06-24T18:14:41.154254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = open('./train_controlnet.py','r').read()\ntext.replace('transforms.Resize(args.resolution', 'transforms.Resize((768,512)')\ntext.replace('transforms.CenterCrop(args.resolution', 'transforms.CenterCrop((768,512)')\ntext.replace('if args.resolution % 8 != 0:', 'if args.resolution % 8 != 0 and False:')\nwith open ('./train_controlnet.py','w') as f: \n    f.write(text)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:14:41.156787Z","iopub.execute_input":"2024-06-24T18:14:41.157118Z","iopub.status.idle":"2024-06-24T18:14:41.164232Z","shell.execute_reply.started":"2024-06-24T18:14:41.157089Z","shell.execute_reply":"2024-06-24T18:14:41.163266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\ndataset = datasets.load_dataset(\"EpsilonGreedy/Clothes2Person\")\nlocal_conditioning_images, local_text = [], []\nfor i in range(len(dataset['train'].data['image'])):\n    path = dataset['train'].data['conditioning_image'][i].as_py()['path']\n    if 'dresscode_dresses_033203.jpg' not in path and 'vitonhdtrain_upperbody_09726_00.jpg' not in path and 'dresscode_lowerbody_015727.jpg' not in path and 'dresscode_upperbody_010285.jpg' not in path:\n        continue\n    local_conditioning_images.append(path)\n    local_text.append(dataset['train'].data['text'][i].as_py())\n\ndel dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:14:41.165348Z","iopub.execute_input":"2024-06-24T18:14:41.165672Z","iopub.status.idle":"2024-06-24T18:17:32.877018Z","shell.execute_reply.started":"2024-06-24T18:14:41.165640Z","shell.execute_reply":"2024-06-24T18:17:32.876172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('/kaggle/working/output',exist_ok=True)\n# num_train_epochs:\n# --resume_from_checkpoint latest \\\nlr = 5e-6\nempty_prompts = 0.5\nlr = str(lr)\nempty_prompts = str(empty_prompts)\ncommand = f\"\"\"\naccelerate launch train_controlnet.py \\\n--pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n--output_dir=\"/kaggle/working/output/Clothes2Person\" \\\n--dataset_name=\"EpsilonGreedy/Clothes2Person\" \\\n--learning_rate={lr} \\\n--validation_image \"{local_conditioning_images[0]}\" \"{local_conditioning_images[1]}\" \"{local_conditioning_images[2]}\" \"{local_conditioning_images[3]}\" \\\n--validation_prompt \"{local_text[0]}\" \"{local_text[1]}\" \"{local_text[2]}\" \"{local_text[3]}\" \\\n--train_batch_size=3 \\\n--gradient_accumulation_steps=4 \\\n--validation_steps=2812 \\\n--gradient_checkpointing \\\n--use_8bit_adam \\\n--proportion_empty_prompts={empty_prompts} \\\n--dataloader_num_workers=4 \\\n--push_to_hub \\\n--hub_model_id=\"EpsilonGreedy/Clothes2Person\" \\\n--checkpointing_steps=2812 \\\n--num_train_epochs=1 \\\n--lr_scheduler=\"constant_with_warmup\" \\\n--lr_warmup_steps=0 \\\n--resume_from_checkpoint latest\n\"\"\"\n# --max_train_steps=4 \\\n# --controlnet_model_name_or_path=\"\"\n# --resume_from_checkpoint latest\n# --revision\n# --hub_token=\"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\" \\\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:45:28.183032Z","iopub.execute_input":"2024-06-24T18:45:28.184081Z","iopub.status.idle":"2024-06-24T19:02:54.257952Z","shell.execute_reply.started":"2024-06-24T18:45:28.184023Z","shell.execute_reply":"2024-06-24T19:02:54.256646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if lr == '5e-6':\n#     lr = ''\n# if empty_prompts == '0.5':\n#     empty_prompts = ''\ncommand = command.replace('hub_model_id=\"EpsilonGreedy/Clothes2Person\"','hub_model_id=\"EpsilonGreedy/Clothes2Person'+lr+empty_prompts+'\"')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!{command}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Using the 16gb tutorial setup: \n# faster 9h/ 2.85s per iter\n# Using 16gb + 12 gb --enable_xformers_memory_efficient_attention --set_grads_to_none is a lot slower\n# Steps:   1%|        | 100/12500 [04:46<9:48:04,  2.85s/it, loss=0.0195, lr=1e-5\n# so we use 16gb config\n# use 2 T4 gpu:\n# Steps:   1%|         | 13/2084 [03:11<8:16:03, 14.37s/it, loss=0.00271, lr=1e-5]\n# can increase batchsize to 3 per node and 4 gradient accumulation steps: effectively batch size of 24\n# - the code can achieve the speedup because of bitsandbites 8bitAdam, where we trade off speed for lower VRam usage\n# - Gradient accumulation, only the compute graph is stored for each intermediate batch, after 4 batches gradients are jointly \"backpropagated\".\n# -  therefore during the forward pass, only 1 of 4 batches is in memory simulatneously, which reduces the maximum memory usage and allows to train with larger effective batch sizes.\n# According to the user https://github.com/lllyasviel (author of the original controlnet implementation for stable diffusion), training on larger batch sizes is prefered \n# to training for more steps. \n# batch size 24 is already \n# total of 50000 images,\n# grad accumulation: 12500 steps, \n# 2 gpu: 6250 steps: \n# bs 3 instead of 1: 2084 steps\n# --checkpointing_steps=1000\n# --resume_from_checkpoint latest\n#  --push_to_hub\n# runwayml/stable-diffusion-v1-5\n# https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet.py\n# --proportion_empty_prompts=0.5\n# --resolution=(768,512) \\\n# !accelerate launch train_controlnet.py \\\n# --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n# --output_dir=\"/kaggle/working/output/trained\" \\\n# --dataset_name=\"EpsilonGreedy/Clothes2Person\" \\\n# --learning_rate=5e-6 \\\n# --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n# --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n# --train_batch_size=3 \\\n# --gradient_accumulation_steps=4 \\\n# --validation_steps=10 \\\n# --gradient_checkpointing \\\n# --use_8bit_adam \\\n# --proportion_empty_prompts=0 \\\n# --dataloader_num_workers=4 \\\n# --push_to_hub \\\n# --hub_token=\"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\" \\\n# --hub_model_id=\"EpsilonGreedy/Clothes2Person\" \\\n# --max_train_steps=40\n# --controlnet_model_name_or_path=\"\"\n# -resume_from_checkpoint latest\n# --revision\n# !accelerate launch train_controlnet.py\\\n# #   ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n# #--resume_from_checkpoint # Using a checkpoint for inference requires separate loading of the original pipeline and the individual checkpointed model components.\"\"See https://huggingface.co/docs/diffusers/main/en/training/dreambooth#performing-inference-using-a-saved-checkpoint for step by step\"\n# --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\"\\\n# # Path to pretrained model or model identifier from huggingface.co/models.\n# #--controlnet_model_name_or_path=\"\"\\Path to pretrained controlnet model or model identifier from huggingface.co/models.If not specified controlnet weights are initialized from unet.\n# # --revision # Revision of pretrained model identifier from huggingface.co/models.\n# # --max_train_steps\n# --output_dir=\"/kaggle/working/output/trained\"\\\n# --dataset_name=\"EpsilonGreedy/Clothes2Person\"\\\n# --resolution=(768,512)\\ \n# # The resolution for input images, all the images in the train/validation dataset will be resized to this\n# --learning_rate=5e-6\\\n# --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\"\\\n# --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\"\\\n# --train_batch_size=3\\\n# --gradient_accumulation_steps=4\\\n# --validation_steps=1000\\\n# --gradient_checkpointing\\\n# --use_8bit_adam\\\n# --proportion_empty_prompts=0\\ \n# #0.5 = 50% \n# #--lr_warmup_steps=500\\ # default\n# #--learning_rate\n# --dataloader_num_workers=4\\\n\n# --push_to_hub\\# action=store_true\n# --hub_token=\"EpsilonGreedy/Clothes2Person\" # the token to use to push to the Model Hub.\n# #--hub_model_id # The name of the repository to keep in sync with the local `output_dir`.\n# #--report_to default: tensorboard\n# # --max_train_samples\n# # --num_validation_images Number of images to be generated for each `--validation_image`, `--validation_prompt` pair","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:17:33.304898Z","iopub.status.idle":"2024-06-24T18:17:33.305258Z","shell.execute_reply.started":"2024-06-24T18:17:33.305088Z","shell.execute_reply":"2024-06-24T18:17:33.305105Z"},"trusted":true},"execution_count":null,"outputs":[]}]}